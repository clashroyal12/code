#!pip install pandas numpy matplotlib seaborn scikit-learn       
#[for py remove !and run in terminal not in code]

# -------------------------------------------------------------
#  PCA (Principal Component Analysis)
# -------------------------------------------------------------

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# -----------------------------
# Load and preprocess dataset
# -----------------------------
iris = load_iris()
X = iris.data
y = iris.target
target_names = iris.target_names

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# -----------------------------
# Apply PCA (2 Components)
# -----------------------------
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# Create DataFrame for plotting
df_pca = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])
df_pca['Target'] = [target_names[i] for i in y]

# -----------------------------
# Plot 2D PCA Scatter (Matplotlib + Seaborn)
# -----------------------------
plt.figure(figsize=(8,6))
sns.scatterplot(
    x='PC1', y='PC2',
    hue='Target',
    palette='Set2',
    data=df_pca,
    s=100,
    edgecolor='k'
)

# Customize
plt.title(" PCA - 2D Scatter Plot (Iris Dataset)", fontsize=14)
plt.xlabel(f"Principal Component 1 ({pca.explained_variance_ratio_[0]*100:.2f}% Variance)")
plt.ylabel(f"Principal Component 2 ({pca.explained_variance_ratio_[1]*100:.2f}% Variance)")
plt.legend(title="Target Classes")
plt.grid(True)
plt.show()

# -----------------------------
# Print Explained Variance
# -----------------------------
print("\nðŸ”¹ Explained Variance Ratio per Component:")
for i, ratio in enumerate(pca.explained_variance_ratio_, 1):
    print(f"  PC{i}: {ratio:.2%}")
print(f" Total Variance Retained: {pca.explained_variance_ratio_.sum():.2%}")









##This graph shows the result of applying Principal Component Analysis (PCA) on the Iris dataset, reducing its four original features to two new components â€” Principal Component 1 (PC1) and Principal Component 2 (PC2). The X-axis (PC1) explains about 72.96% of the total variance, and the Y-axis (PC2) explains 22.85%, meaning together they retain around 95.8% of the datasetâ€™s information. Each point represents one flower sample, and the colors correspond to the three Iris species â€” Setosa, Versicolor, and Virginica. The plot clearly shows that Setosa forms a distinct, well-separated cluster, while Versicolor and Virginica overlap slightly, indicating they have more similar features. Overall, the graph demonstrates how PCA effectively reduces dimensions while preserving most of the important patterns and relationships in the data.##
